{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNfshiI3qogdFoML1juV6qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aljebraschool/ai-startup-idea-generator/blob/master/LLM_university_Introduction_to_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "fGBAs1CcZ9ts"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade cohere -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere"
      ],
      "metadata": {
        "id": "XWIyFNLnYNZ1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.ClientV2(\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ],
      "metadata": {
        "id": "XImS8s4NXxGj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Documents\n",
        "\n",
        "Next, we define the documents that we want to ground an LLM’s response with, formatted as a list. In our case, each document consists of two fields: title and text.\n",
        "\n",
        "The documents list includes a list of documents with a “text” field containing the information we want the model to use. The recommended length for the snippet of each document is relatively short, 300 words or less. We recommend using field names similar to the ones we’ve included in this example (i.e., “title” and “text”), but RAG is quite flexible with respect to how you structure the documents listings. You can give the fields any names you want, and you can pass in other fields as well, such as a “date” field. All field names and field values are passed to the model."
      ],
      "metadata": {
        "id": "jO9bYqm9Yl5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    {\"title\": \"Who is Ridwan Ibidunni\",\n",
        "     \"text\": \"Ridwan is a mathematician with several years of experience programming and solving problems using code\"},\n",
        "    {\"title\": \"Programming languages Ridwan loves\",\n",
        "     \"text\": \"He loves coding in Java, Python and using machine learning frameworks like PyTorch, Tensorflow and scikit-learn\"}\n",
        "\n",
        "  ]\n"
      ],
      "metadata": {
        "id": "H0LZ_f0JYL5i"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate the Response with Citations\n",
        "\n",
        "Cohere’s RAG functionalities are part of the Chat endpoint, with the Command model as the underlying LLM. This allows developers to build chatbots that have the full context of a conversation and are not limited to a single interaction.\n",
        "\n",
        "First, we define the message coming from the user. We’ll use a simple query,"
      ],
      "metadata": {
        "id": "uj_jvYEfZ75L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we pass this message as a message parameter to a Chat endpoint call. We also pass the list of documents as a documents parameter. By using the chat_stream method, the response is generated incrementally by token without having to wait for the full completion."
      ],
      "metadata": {
        "id": "IURHUR4AapKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we display the response from the model. The streamed response will return different types of objects, and for now, we are interested in the text-generation objects, which contain the generated text.\n",
        "\n",
        "We also display the citations and source documents, which we can get from the final object returned by the streamed response."
      ],
      "metadata": {
        "id": "ARABRFBbbz8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the user message\n",
        "prompt = \"Does he loves maths\"\n",
        "\n",
        "# Generate the response\n",
        "response = co.chat_stream(\n",
        "    message = prompt,\n",
        "    documents=documents,\n",
        "    model=\"command-r-plus\",\n",
        ")\n",
        "\n",
        "\n",
        "#display the response\n",
        "citations = []\n",
        "cited_document = []\n",
        "\n",
        "for event in response:\n",
        "    if event.event_type == \"text-generation\":\n",
        "        print(event.text, end=\"\")\n",
        "    elif event.event_type == \"citation-generation\":\n",
        "        citations.extend(event.citations)\n",
        "    elif event.event_type == \"stream-end\":\n",
        "      cited_documents = event.response.documents\n",
        "\n",
        "# Display the citations and source documents\n",
        "if citations:\n",
        "  print(\"\\n\\nCITATIONS:\")\n",
        "  for citation in citations:\n",
        "    print(citation)\n",
        "\n",
        "  print(\"\\nDOCUMENTS:\")\n",
        "  for document in cited_documents:\n",
        "    print(document)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rITL2p9dbo0p",
        "outputId": "e6bcd4b6-dd3c-41b4-f8e6-8d948340dddd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, Ridwan Ibidunni is a mathematician with several years of experience programming and solving problems using code. He loves coding in Java and Python and using machine learning frameworks like PyTorch, Tensorflow and scikit-learn.\n",
            "\n",
            "CITATIONS:\n",
            "start=5 end=20 text='Ridwan Ibidunni' document_ids=['doc_0']\n",
            "start=26 end=39 text='mathematician' document_ids=['doc_0']\n",
            "start=45 end=117 text='several years of experience programming and solving problems using code.' document_ids=['doc_0']\n",
            "start=127 end=152 text='coding in Java and Python' document_ids=['doc_1']\n",
            "start=157 end=233 text='using machine learning frameworks like PyTorch, Tensorflow and scikit-learn.' document_ids=['doc_1']\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Ridwan is a mathematician with several years of experience programming and solving problems using code', 'title': 'Who is Ridwan Ibidunni'}\n",
            "{'id': 'doc_1', 'text': 'He loves coding in Java, Python and using machine learning frameworks like PyTorch, Tensorflow and scikit-learn', 'title': 'Programming languages Ridwan loves'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pq9sfA2YnYUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}