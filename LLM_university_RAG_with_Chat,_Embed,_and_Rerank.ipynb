{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTAbB2CgEgQ9LxKGs+c1ck",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aljebraschool/ai-startup-idea-generator/blob/master/LLM_university_RAG_with_Chat%2C_Embed%2C_and_Rerank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "First, let’s import the necessary libraries for this project. This includes cohere, hnswlib for the vector library, and unstructured for chunking the documents"
      ],
      "metadata": {
        "id": "zFCfxeaO-3hK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cohere unstructured hnswlib -q"
      ],
      "metadata": {
        "id": "GsxBSfbj-8VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere\n",
        "import uuid\n",
        "import hnswlib\n",
        "from unstructured.partition.html import partition_html\n",
        "from unstructured.chunking.title import chunk_by_title"
      ],
      "metadata": {
        "id": "QOt6Ed1f_F5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.ClientV2(\"COHERE_API_KEY\") # Get your free API key: https://dashboard.cohere.com/api-keys"
      ],
      "metadata": {
        "id": "0h5dQ7Gl_jqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Vectorstore Component\n",
        "\n",
        "The Vectorstore class handles the ingestion of documents into embeddings (or vectors) and the retrieval of relevant documents given a query.\n",
        "\n",
        "As an example, we’ll use the contents from Cohere's documentation on prompt engineering. It consists of four web pages, each in the Python list raw_documents below. Each entry is identified by its title and URL."
      ],
      "metadata": {
        "id": "JeRbf41K_val"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_documents = [\n",
        "    {\"title\": \"Crafting Effective Prompts\",\n",
        "     \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
        "    {\"title\": \"Advanced Prompt Engineering Techniques\",\n",
        "     \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
        "    {\"title\": \"Prompt Truncation\",\n",
        "     \"url\": \"https://docs.cohere.com/docs/prompt-truncation\"},\n",
        "    {\"title\": \"Preambles\",\n",
        "     \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "XHQE7h-m_qxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We implement this in the Vectorstore class below, which takes the raw_documents list as input.\n",
        "\n",
        "We also initialize a few instance attributes and methods. The attributes include self.raw_documents to represent the raw documents, self.docs to represent the chunked version of the documents, self.docs_embs to represent the embeddings of the chunked documents, and a couple of top_k parameters to be used for retrieval and reranking.\n",
        "\n",
        "Meanwhile, the methods include load_and_chunk(), embed(), and index() for ingesting raw documents. As you’ll see, we will also specify a retrieve() method to retrieve relevant document chunks given a query."
      ],
      "metadata": {
        "id": "8E0rET3tBrDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Chunk the Documents\n",
        "\n",
        "The load_and_chunk() method loads the raw documents from the URL and breaks them into smaller chunks. Chunking for information retrieval is a broad topic in and of itself, with many strategies being discussed within the AI community. For our example, we’ll utilize the partition_html method from the unstructured library.\n",
        "\n",
        "Each chunk is turned into a dictionary with three fields:\n",
        "\n",
        "title: The web page’s title\n",
        "text: The textual content of the chunk\n",
        "url: The web page’s URL\n",
        "\n",
        "This information will eventually be passed to the chatbot’s prompt for generating the response, so it’s crucial to populate relevant information into this dictionary. Note that we are not limited to these three fields. At a minimum, the Chat endpoint requires the text field, but beyond that, we can add custom fields that can provide more context about the document, such as subtitles, snippets, tags, and others.\n",
        "\n",
        "The resulting dictionaries are stored in the self.docs attribute."
      ],
      "metadata": {
        "id": "-SyZNKBOFbc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embed the Document Chunks\n",
        "\n",
        "The embed() method generates embeddings of the chunked documents. We use the Embed endpoint and Cohere's embed-english-v3.0 model. Since the endpoint has a limit of 96 documents per call, we send them in batches.\n",
        "\n",
        "With the Embed v3 model, we need to define an input_type, of which there are four options depending on the type of task. Using these input types ensures the highest possible quality for the respective tasks. Since our document chunks will be used for retrieval, we use search_document as the input_type.\n",
        "\n",
        "The resulting chunk embeddings are stored in the self.docs_embs attribute."
      ],
      "metadata": {
        "id": "UNuo4fMlFlhj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index Document Chunks\n",
        "\n",
        "The index() method indexes the document chunk embeddings. We build an index to store the embeddings in a structured and organized way in order to ensure efficient similarity search during retrieval.\n",
        "\n",
        "There are many options available for building an index. For production environments, typically a vector database (like Weaviate or MongoDB) is required to handle the continuous process of indexing documents and maintaining the index.\n",
        "\n",
        "In our example, however, we’ll keep it simple and use a vector library instead. We can choose from many open-source projects, such as Faiss, Annoy, ScaNN, or Hnswlib, which is the one we’ll use. These libraries store embeddings in in-memory indexes and implement approximate nearest neighbor (ANN) algorithms to make similarity search efficient.\n",
        "\n",
        "The resulting document chunk embeddings are stored in the self.idx attribute."
      ],
      "metadata": {
        "id": "jy82scTiIyB2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement Retrieval\n",
        "\n",
        "The retrieve() method uses semantic search to retrieve relevant document chunks given a query, and it has two steps: (1) dense retrieval, (2) reranking.\n",
        "\n",
        "# Dense Retrieval\n",
        "\n",
        "We implement a dense retrieval system that leverages embeddings to retrieve document chunks, offering significant improvements over basic keyword-matching approaches. Embeddings can capture the contextual meaning of a document, thus enabling the retrieval of highly relevant results to the given query.\n",
        "\n",
        "We embed the query using the same embed-english-v3.0 model that we used to embed the document chunks, but this time, we set input_type=”search_query”.\n",
        "\n",
        "Search is performed by the knn_query() method from the hnswlib library. Given a query, it returns the document chunks most similar to the query. We define the number of document chunks to return using the attribute self.retrieve_top_k=10.\n",
        "\n",
        "# Reranking\n",
        "After dense retrieval, we implement a reranking step. While our dense retrieval component is already highly capable of retrieving relevant sources, Cohere Rerank eprovides an additional boost to the quality of the search results, especially for complex and domain-specific queries. It takes the search results and sorts them according to their relevance to the query.\n",
        "\n",
        "We call the Rerank endpoint with co.rerank() and pass the query and the list of document chunks to be reranked. We also define the number of top reranked document chunks to retrieve using the attribute self.rerank_top_k=3. The model we use is rerank-english-v3.0, which lets you rerank documents that contain multiple fields, in the form of JSON objects. In our case, we'll use the title and text fields for reranking.\n",
        "\n",
        "This method returns the top retrieved document chunks as a Python list docs_retrieved, so that they can be passed to the chatbot, which we’ll implement next."
      ],
      "metadata": {
        "id": "RG4N4BVgK5j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vectorstore:\n",
        "    \"\"\"\n",
        "    A class representing a collection of documents indexed into a vectorstore.\n",
        "\n",
        "    Parameters:\n",
        "    raw_documents (list): A list of dictionaries representing the sources of the raw documents. Each dictionary should have 'title' and 'url' keys.\n",
        "\n",
        "    Attributes:\n",
        "    raw_documents (list): A list of dictionaries representing the raw documents.\n",
        "    docs (list): A list of dictionaries representing the chunked documents, with 'title', 'text', and 'url' keys.\n",
        "    docs_embs (list): A list of the associated embeddings for the document chunks.\n",
        "    docs_len (int): The number of document chunks in the collection.\n",
        "    idx (hnswlib.Index): The index used for document retrieval.\n",
        "\n",
        "    Methods:\n",
        "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
        "    embed(): Embeds the document chunks using the Cohere API.\n",
        "    index(): Indexes the document chunks for efficient retrieval.\n",
        "    retrieve(): Retrieves document chunks based on the given query.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, raw_documents : list[dict[str, str]]):\n",
        "      self.raw_documents = raw_documents\n",
        "      self.docs = []\n",
        "      self.docs_embedings = []\n",
        "      self.retrieve_top_k = 10\n",
        "      self.rerank_top_k = 3\n",
        "      self.load_and_chunk()\n",
        "      self.embed()\n",
        "      self.index()\n",
        "\n",
        "    def load_and_chunk(self) -> None:\n",
        "        \"\"\"\n",
        "        Loads the text from the sources and chunks the HTML content.\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"Loading Documents...\")\n",
        "\n",
        "        for raw_documents in self.raw_documents:\n",
        "          elements = partition_html(url=raw_documents['url'])\n",
        "          chunks = chunk_by_title(elements)\n",
        "\n",
        "          for chunk in chunks:\n",
        "            self.docs.append(\n",
        "                {\n",
        "                    \"title\": raw_documents[\"title\"],\n",
        "                    \"text\": str(chunk),\n",
        "                    \"url\": raw_documents[\"url\"]\n",
        "                }\n",
        "            )\n",
        "\n",
        "    def embed(self) -> None:\n",
        "        \"\"\"\n",
        "        Embeds the document chunks using the Cohere API.\n",
        "        \"\"\"\n",
        "        print(\"Embedding documents chunks...\")\n",
        "\n",
        "        batch_size = 90\n",
        "        self.docs_len = len(self.docs)\n",
        "        for i in range(0, self.docs_len, batch_size):\n",
        "          batch = self.docs[i: min(i + batch_size, self.docs_len)]\n",
        "          texts = [item['text'] for item in batch]\n",
        "\n",
        "          response = co.embed(\n",
        "              texts = texts,\n",
        "              model=\"embed-english-v3.0\",\n",
        "              input_type = 'search_document',\n",
        "          ).embeddings\n",
        "          self.docs_embedings.extend(response)\n",
        "\n",
        "    def index(self) -> None:\n",
        "        \"\"\"\n",
        "        Indexes the documents for efficient retrieval.\n",
        "        \"\"\"\n",
        "        print(\"Indexing documents...\")\n",
        "\n",
        "        self.idx = hnswlib.Index(space = 'ip', dim = 1024)\n",
        "        self.idx.init_index(max_elements = self.docs_len, ef_construction = 512, M = 64)\n",
        "        self.idx.add_items(self.docs_embedings, list(range(len(self.docs_embedings))))\n",
        "\n",
        "        print(f\"indexing complete with {self.idx.get_current_count()} documents\")\n",
        "\n",
        "    def retrieve(self, query : str) -> None:\n",
        "        \"\"\"\n",
        "        Retrieves document chunks based on the given query.\n",
        "\n",
        "        Parameters:\n",
        "        query (str): The query to retrieve document chunks for.\n",
        "\n",
        "        Returns:\n",
        "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
        "        \"\"\"\n",
        "\n",
        "        # dense retrieval\n",
        "        query_embedding = co.embed(\n",
        "            texts = [query],\n",
        "            model=\"embed-english-v3.0\",\n",
        "            input_type = 'search_query'\n",
        "        ).embeddings\n",
        "\n",
        "        doc_ids = self.idx.knn_query(query_embedding, k = self.retrieve_top_k)[0][0]\n",
        "\n",
        "        #Reranking\n",
        "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
        "\n",
        "        doc_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
        "\n",
        "        rerank_result = co.rerank(\n",
        "            query = query,\n",
        "            documents = doc_to_rerank,\n",
        "            top_n = self.rerank_top_k,\n",
        "            model = 'rerank-english-v3.0',\n",
        "            rank_fields = rank_fields\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "        docs_ids_reranked = [doc_ids[result.index]  for result in rerank_result.results]\n",
        "\n",
        "        docs_retrieved = []\n",
        "\n",
        "        for doc_id in docs_ids_reranked:\n",
        "          docs_retrieved.append(\n",
        "              {\"title\": self.docs[doc_id]['title'],\n",
        "               \"text\": self.docs[doc_id][\"text\"],\n",
        "               \"url\": self.docs[doc_id][\"url\"]\n",
        "\n",
        "\n",
        "               }\n",
        "\n",
        "          )\n",
        "\n",
        "        return docs_retrieved\n",
        "\n"
      ],
      "metadata": {
        "id": "ii6-X9j4BrkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process the Documents\n",
        "\n",
        "We can now process the raw documents. We do that by creating an instance of Vectorstore. In our case, we get a total of 136 documents, chunked from the four web URLs."
      ],
      "metadata": {
        "id": "BeJ7bKVFXHji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Vectorstore(raw_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxsTDDx4DYzT",
        "outputId": "5811f0ce-b961-4828-95d0-1cb1390b2c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Documents...\n",
            "Embedding documents chunks...\n",
            "Indexing documents...\n",
            "indexing complete with 105 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Retrieval\n",
        "\n",
        "Before going further, we first test the document retrieval part of the system. First, we create an instance of the Vectorstore with the raw documents that we have defined. Then, we use the retrieve method to retrieve the most relevant documents to the query \"Prompting by giving examples.\""
      ],
      "metadata": {
        "id": "g9UGLEreE4iG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.retrieve(\"Prompting by giving examples\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wx3kaNyDE1N8",
        "outputId": "62f75dd5-ac59-4b78-8e08-68d5d2bb6f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Advanced Prompt Engineering Techniques',\n",
              "  'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.',\n",
              "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'},\n",
              " {'title': 'Crafting Effective Prompts',\n",
              "  'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',\n",
              "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'},\n",
              " {'title': 'Advanced Prompt Engineering Techniques',\n",
              "  'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.',\n",
              "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now run the chatbot. For this, we create a generate_chat function which includes the RAG components:\n",
        "- For each user message, we use the endpoint’s search query generation feature to turn the message into one or more queries that are optimized for retrieval. The endpoint can even return no query, which means that a user message can be responded to directly without retrieval. This is done by calling the Chat endpoint with the search_queries_only parameter and setting it as True.\n",
        "- If there is no search query generated, we call the Chat endpoint to generate a response directly. If there is at least one, we call the retrieve method from the Vectorstore instance to retrieve the most relevant documents to each query.\n",
        "- Finally, all the results from all queries are appended to a list and passed to the Chat endpoint for response generation.\n",
        "- We print the response, together with the citations and the list of document chunks cited, for easy reference."
      ],
      "metadata": {
        "id": "gHBkJa7VZn6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_chatbot(message, chat_history=[]):\n",
        "  # Generate search queries, if any\n",
        "  response = co.chat(message = message,\n",
        "                     model=\"command-r-plus\",\n",
        "                     search_queries_only=True,\n",
        "                     chat_history=chat_history\n",
        "                     )\n",
        "\n",
        "  search_queries = []\n",
        "  for query in response.search_queries:\n",
        "    search_queries.append(query.text)\n",
        "\n",
        "  # If there are search queries, retrieve the documents\n",
        "  if search_queries:\n",
        "    print(\"Retrieving Information...\",  end = '')\n",
        "\n",
        "    # Retrieve document chunks for each query\n",
        "    docs_retrieved = []\n",
        "    for query in search_queries:\n",
        "      docs_retrieved.extend(vectorstore.retrieve(query))\n",
        "\n",
        "    # Use document chunks to respond\n",
        "    response = co.chat_stream(\n",
        "        message = message,\n",
        "        model=\"command-r-plus\",\n",
        "        documents = docs_retrieved,\n",
        "        chat_history=chat_history\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    response = co.chat_stream(\n",
        "        message = message,\n",
        "        model=\"command-r-plus\",\n",
        "        chat_history=chat_history\n",
        "    )\n",
        "\n",
        "  # Print the chatbot response and citations\n",
        "  chatbot_response = \"\"\n",
        "  print(\"\\nChatbot:\")\n",
        "\n",
        "  for event in response:\n",
        "    if event.event_type == \"text-generation\":\n",
        "      print(event.text, end = \"\")\n",
        "      chatbot_response += event.text\n",
        "\n",
        "    if event.event_type == \"stream-end\":\n",
        "      if event.response.citations:\n",
        "        print(\"\\n\\nCITATIONS:\")\n",
        "        for citation in event.response.citations:\n",
        "          print(citation)\n",
        "\n",
        "      if event.response.documents:\n",
        "        print(\"\\n\\nDOCUMENTS:\")\n",
        "        for document in event.response.documents:\n",
        "          print(document)\n",
        "\n",
        "      # Update the chat history for the next turn\n",
        "      chat_history = event.response.chat_history\n",
        "\n",
        "  return chat_history\n"
      ],
      "metadata": {
        "id": "L_eGLy1gZMiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Query Generation\n",
        "Let's take a deeper look at the search query generation feature. Based on the user message, the chatbot needs to decide if it needs to consult external information before responding. If so, the chatbot determines an optimal set of search queries to use for retrieval. When we call co.chat() with search_queries_only=True, the Chat endpoint handles this for us automatically.\n",
        "\n",
        "The generated queries can be accessed from the search_queries field of the object that is returned. To understand how this works, let’s look at a few scenarios:\n",
        "\n",
        "No query needed: Suppose we have a user message of “Hello, I need help with a report I'm writing”. This type of message doesn’t require any additional context from external information, so retrieval is not required. A direct chatbot response will suffice (for example: “Sure, how can I help?”). When we send this to the Chat endpoint, we get an empty search_queries result, which is what we expect.\n",
        "One query generated: Take this user message: \"What did the report say about the company's Q4 performance?” This does require additional context as it refers to a report, hence retrieval is required. Given this message, the Chat endpoint returns the search_queries result of Q4 company performance. Here it turns the user message into a query optimized for search. Another important scenario is generating queries in the context of the conversation. Suppose there’s an ongoing conversation where the user is learning from the chatbot about deep learning. If at some point, the user asks, “Why is it important”, then the generated search_queries will become why is deep learning important, providing the much-needed context for the retrieval process.\n",
        "More than one query generated: What if the user message is a bit more complex, such as \"What did the report say about the company's Q4 performance and its range of products and services?” This requires multiple pieces of information to be retrieved. Given this message, the Chat endpoint returns two search_queries results: Q4 company performance and company's range of products and services.\n",
        "\n",
        "These scenarios highlight the adaptability of the Chat endpoint to decide on the next course of action based on a user message."
      ],
      "metadata": {
        "id": "nWoQB1f4feuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Retrieval\n",
        "Let's take a deeper look at the document retrieval step. What happens next depends on how many search queries are returned.\n",
        "\n",
        "If search queries are returned\n",
        "\n",
        "If the chatbot response contains at least one search query, we call the retrieve() method from the Vectorstore class instance to retrieve document chunks that are relevant to the queries.\n",
        "\n",
        "Then, we call the Chat endpoint to generate a response, adding a documents parameter to the call to pass the relevant document chunks.\n",
        "\n",
        "If no search queries are returned\n",
        "\n",
        "Meanwhile, if the chatbot response doesn’t contain any search queries, then it doesn’t require information retrieval. To generate the response, we call the Chat endpoint another time, passing the user message and without needing to add any sources to the call.\n",
        "\n",
        "In either case, we also pass the chat_history parameter, which retains the interactions between the user and the chatbot in the same conversation thread. We also use the chat_stream endpoint so we can stream the chatbot response to the application."
      ],
      "metadata": {
        "id": "hA93s5bogCfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Response and Citation Generation\n",
        "Let's take a deeper look at the response generation step. The chatbot response includes a stream of events, such as the generated text and citations followed by a final object which contains the sources used by the chatbot along with other details.\n",
        "\n",
        "To display the response, we use the text-generation events from the response stream.\n",
        "\n",
        "The citation-generation events indicate the spans of text from the retrieved document chunks on which the response is grounded. Here is one example:\n",
        "\n",
        "\n",
        "\n",
        "> start=382 end=397 text='similar vectors' document_ids=['doc_0', 'doc_2']\n",
        "\n",
        "The format of each citation is:\n",
        "\n",
        "start: The starting point of a span where one or more documents are referenced\n",
        "end: The ending point of a span where one or more documents are referenced\n",
        "text: The text representing this span\n",
        "document_ids: The IDs of the document chunks being referenced (doc_0 being the ID of the first document chunk passed to the documents creating parameter in the endpoint call, and so on)\n",
        "The final response object includes a list of the document chunks, which we access from the documents attribute.\n",
        "\n"
      ],
      "metadata": {
        "id": "G4gygRLYgSyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example conversation\n",
        "Here’s an example of a conversation that happens over a few turns:"
      ],
      "metadata": {
        "id": "_nDl45p3gklf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 1\n",
        "\n",
        "chat_history = run_chatbot(\"Hello, I have a question!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiuLyy4ueYh-",
        "outputId": "b3c098d7-4e61-40dc-b693-1c2e5ca8b786"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chatbot:\n",
            "Of course! I am here to help. Please go ahead with your question and I will do my best to assist you."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 2\n",
        "chat_history = run_chatbot(\"what is the different between zero-shot and few-shot prompting?\", chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muuxaG2vgx85",
        "outputId": "028df720-a359-4cc3-c53a-c131f599b1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving Information...\n",
            "Chatbot:\n",
            "Zero-shot prompting is a technique where a model is asked to perform a task without being provided with any examples. On the other hand, few-shot prompting involves providing a model with a few relevant examples of the task being performed before asking the specific question to be answered. This helps steer the model toward a high-quality solution.\n",
            "\n",
            "CITATIONS:\n",
            "start=0 end=19 text='Zero-shot prompting' document_ids=['doc_0', 'doc_3']\n",
            "start=43 end=117 text='model is asked to perform a task without being provided with any examples.' document_ids=['doc_0', 'doc_3']\n",
            "start=137 end=155 text='few-shot prompting' document_ids=['doc_0', 'doc_3']\n",
            "start=165 end=239 text='providing a model with a few relevant examples of the task being performed' document_ids=['doc_0', 'doc_3']\n",
            "start=258 end=291 text='specific question to be answered.' document_ids=['doc_0', 'doc_3']\n",
            "start=303 end=350 text='steer the model toward a high-quality solution.' document_ids=['doc_0', 'doc_3']\n",
            "\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n",
            "{'id': 'doc_3', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 3\n",
        "chat_history = run_chatbot(\"how will the latter help?\", chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYUprll1hDyh",
        "outputId": "8a906471-3511-4bc8-e649-f9223a74faa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving Information...\n",
            "Chatbot:\n",
            "Few-shot prompting can vastly improve the quality of a model's completions. Providing a few relevant and diverse examples in the prompt helps steer the model toward a high-quality solution by conditioning it to the expected response type and style.\n",
            "\n",
            "CITATIONS:\n",
            "start=23 end=75 text=\"vastly improve the quality of a model's completions.\" document_ids=['doc_1']\n",
            "start=88 end=121 text='few relevant and diverse examples' document_ids=['doc_0']\n",
            "start=142 end=188 text='steer the model toward a high-quality solution' document_ids=['doc_0']\n",
            "start=192 end=248 text='conditioning it to the expected response type and style.' document_ids=['doc_0']\n",
            "\n",
            "\n",
            "DOCUMENTS:\n",
            "{'id': 'doc_1', 'text': 'Advanced Prompt Engineering Techniques\\n\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n",
            "{'id': 'doc_0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn # 4\n",
        "chat_history = run_chatbot(\"What do you know about 5G network?\", chat_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gKHfreMhb4l",
        "outputId": "f9b82c7f-fe19-46d5-917e-31b3146e275d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving Information...\n",
            "Chatbot:\n",
            "Sorry, I do not have access to any information about the 5G network. Can I help you with anything else?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a few observations worth pointing out:\n",
        "\n",
        "- Direct response: For user messages that don’t require retrieval (“Hello, I have a question”), the chatbot responds directly without requiring retrieval.\n",
        "- Citation generation: For responses that do require retrieval (\"What's the difference between zero-shot and few-shot prompting\"), the endpoint returns the response together with the citations. These are fine-grained citations, which means they refer to specific spans of the generated text.\n",
        "- State management: The endpoint maintains the state of the conversation via the chat_history parameter, for example, by correctly responding to a vague user message such as \"How would the latter help?\"\n",
        "- Response synthesis: The model can decide if none of the retrieved documents provide the necessary information to answer a user message. For example, when asked the question, “What do you know about 5G networks”, the chatbot retrieves external information from the index. However, it doesn’t use any of the information in its response as none of it is relevant to the question."
      ],
      "metadata": {
        "id": "9iz32UyziJq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Chat history:\")\n",
        "for c in chat_history:\n",
        "    print(c, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8lKFu3Zhtwa",
        "outputId": "7f84ba02-f9ad-4d24-f13c-313546e93b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chat history:\n",
            "role='USER' message='Hello, I have a question!' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message='Of course! I am here to help. Please go ahead with your question and I will do my best to assist you.' tool_calls=None \n",
            "\n",
            "role='USER' message='what is the different between zero-shot and few-shot prompting?' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message='Zero-shot prompting is a technique where a model is asked to perform a task without being provided with any examples. On the other hand, few-shot prompting involves providing a model with a few relevant examples of the task being performed before asking the specific question to be answered. This helps steer the model toward a high-quality solution.' tool_calls=None \n",
            "\n",
            "role='USER' message='how will the latter help?' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message=\"Few-shot prompting can vastly improve the quality of a model's completions. Providing a few relevant and diverse examples in the prompt helps steer the model toward a high-quality solution by conditioning it to the expected response type and style.\" tool_calls=None \n",
            "\n",
            "role='USER' message='What do you know about 5G network?' tool_calls=None \n",
            "\n",
            "role='CHATBOT' message='Sorry, I do not have access to any information about the 5G network. Can I help you with anything else?' tool_calls=None \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P7m9QkpIiOAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}