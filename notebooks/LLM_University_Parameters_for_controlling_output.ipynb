{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# In this chapter, you’ll learn about the parameters that you can leverage to ​​control the Chat endpoint's outputs."
      ],
      "metadata": {
        "id": "312nWWkXnily"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuLesgyQl_Ue",
        "outputId": "2f54dee2-53b6-401d-e849-6007b60f2431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.13.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.27.2)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting parameterized<0.10.0,>=0.9.0 (from cohere)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.9.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.23.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.10/dist-packages (from cohere) (0.20.3)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.21.2->cohere) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<1,>=0.15->cohere) (0.26.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.66.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.2.2)\n",
            "Downloading cohere-5.13.0-py3-none-any.whl (249 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.7/249.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.9.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, parameterized, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.13.0 fastavro-1.9.7 httpx-sse-0.4.0 parameterized-0.9.0 types-requests-2.32.0.20241016\n"
          ]
        }
      ],
      "source": [
        "!pip install cohere"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cohere"
      ],
      "metadata": {
        "id": "5MMCYPdkmO3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "co = cohere.ClientV2(\"egFS8BVGz1DDbYewbj0Tjzh7fUByUIEzflL2GCsM\")"
      ],
      "metadata": {
        "id": "qdUbEgPpmq0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Type\n",
        "\n",
        "With the Chat endpoint , you can choose from several variations of the Command model . Different models may produce different output profiles, so you may want to experiment with different models to get the best output for your use case.\n",
        "\n",
        "Use the model parameter to select a variation that suits your requirements. In the code cell, we select command-r-plus-08-2024."
      ],
      "metadata": {
        "id": "cp1-VE1EnosT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#add user message\n",
        "message = \"hello\"\n",
        "\n",
        "response = co.chat(model = \"command-r-plus-08-2024\" , messages=[{\"role\": \"user\", \"content\": message}])\n",
        "\n",
        "print(response.message.content[0].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmwjcJi8mxZn",
        "outputId": "5a5b1b84-1a52-4742-8235-026e5039acb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomness\n",
        "\n",
        "Often, you’ll need to control the level of randomness of the model. There are a number of factors to keep in mind when tuning model randomness, including:\n",
        "\n",
        "Task type: You’ll likely want to decrease the randomness of the model when performing structured tasks that have a correct answer, like for question answering or summarization, or for generating technical text. In these cases, we want the model to yield a safe and predictable response. On the other hand, if you’re generating poetry or brainstorming ideas, you might want to increase the randomness to produce more diverse and creative responses.\n",
        "Model behavior: We likely need to increase the randomness of the model if it gets stuck in a loop and starts repeating itself, or if it is producing overly generic phrases. Increasing the randomness will expand the set of words that the model can use when generating responses.\n",
        "Controlling style and tone: If generating text that needs to have a specific tone or style, like for a company blog post or customer support responses, a low level of randomness might be desirable, to keep the model from generating unusual words.\n",
        "Modifying the temperature parameter changes the extent to which the model considers incorporating unlikely tokens (can be words, parts of words, or punctuation) in its response, which can make the output more random and creative.\n",
        "\n",
        "The temperature parameter is a value between 0 and 1. As you increase the temperature, the model gets more creative and random. Temperature can be tuned for different problems, and most people will find that the default temperature of 0.3 is a good starting point.\n",
        "\n",
        "Here's an example of using a low temperature value and running three generations. The result is the same each time."
      ],
      "metadata": {
        "id": "dtZZjKqPoaCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the user message\n",
        "message = \"I like learning about the industrial revolution and how it shapes the modern world. How can I introduce myself in two words.\"\n",
        "\n",
        "# Generate the response multiple times by specifying a low temperature value\n",
        "for idx in range(3):\n",
        "    response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                       messages=[{'role':'user', 'content': message}],\n",
        "                       temperature=0)\n",
        "\n",
        "    print(f\"{idx+1}: {response.message.content[0].text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NVZdiEqoEal",
        "outputId": "35af1b7d-4f9e-437f-ddcf-1c381bc3701d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: \"History Enthusiast\"\n",
            "\n",
            "2: \"History Enthusiast\"\n",
            "\n",
            "3: \"History Enthusiast\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## However, if we increase the temperature to the maximum value of 1, the model gives different generations."
      ],
      "metadata": {
        "id": "ed3OLZK9pVfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the user message\n",
        "message = \"I like learning about the industrial revolution and how it shapes the modern world. How can I introduce myself in two words.\"\n",
        "\n",
        "# Generate the response multiple times by specifying a high temperature value\n",
        "for idx in range(3):\n",
        "    response = co.chat(model=\"command-r-plus-08-2024\",\n",
        "                       messages=[{'role':'user', 'content': message}],\n",
        "                       temperature=1)\n",
        "\n",
        "    print(f\"{idx+1}: {response.message.content[0].text}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzgBSUZMpRLq",
        "outputId": "6a0861ba-7612-4751-fce6-c5e544ce3c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1: \"History Enthusiast.\"\n",
            "\n",
            "2: Industrial Historian.\n",
            "\n",
            "3: \"Enthusiastic Historian.\"\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSphS1yApbYj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}